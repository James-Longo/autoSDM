#' Generate Niche Similarity Map
#'
#' This function generates a continuous niche similarity map (dot product) in Google Earth Engine
#' based on the species centroid. The map is returned at the specified resolution.
#'
#' @param df Data frame containing the original survey data (Latitude, Longitude) to determine the AOI.
#' @param analysis_meta_path Path to the JSON metadata file generated by `analyze_embeddings`.
#' @param output_dir Directory where the output map and metadata will be saved. Defaults to current working directory.
#' @param scale Resolution in meters for the output map. Defaults to 10.
#' @param coarse_meta_path Optional. Path to metadata for 1km filtering. If provided, analysis is restricted to 1km pixels > 5th percentile.
#' @param gcs_bucket Optional. GCS bucket name for server-side export. If provided, bypasses local download.
#' @param wait Logical. If TRUE, waits for the GCS export task(s) to complete and shows progress updates.
#' @param zip Logical. If TRUE, zips the output rasters into a single archive (local mode only).
#' @return A list containing the paths to the generated map files, HTML, or GCS export details.
#' @keywords internal
extrapolate <- function(df, analysis_meta_path, output_dir = getwd(), scale = 10, coarse_meta_path = NULL, view = FALSE, gcs_bucket = NULL, wait = FALSE, zip = FALSE, python_path = NULL, gee_project = NULL) {
  # Resolve python path using the centralized helper
  py_exe <- resolve_python_path(python_path)

  if (is.null(py_exe) || !file.exists(py_exe)) {
    stop("Could not find a valid Python environment.\nPlease ensure Python is installed and detected by `reticulate::py_config()`, or provide the `python_path` argument explicitly.")
  }

  # Create input temp file
  tmp_in <- tempfile(fileext = ".csv")
  write.csv(df, tmp_in, row.names = FALSE)

  # Define output JSON path in the requested directory
  output_json <- file.path(output_dir, paste0("extrapolation_results_", scale, "m.json"))

  message("Calling Python extrapolation logic at scale ", scale, "m...")
  args <- c(
    "-m", "autoSDM.cli", "extrapolate",
    "--input", shQuote(tmp_in),
    "--output", shQuote(output_json),
    "--meta", shQuote(analysis_meta_path),
    "--scale", scale
  )

  if (!is.null(gee_project) && gee_project != "") {
    args <- c(args, "--project", shQuote(gee_project))
  }


  if (exists("sa_json_key") && !is.null(sa_json_key) && sa_json_key != "") {
    args <- c(args, "--key", shQuote(sa_json_key))
  }

  if (!is.null(coarse_meta_path)) {
    args <- c(args, "--coarse-meta", shQuote(coarse_meta_path))
  }

  if (view) {
    args <- c(args, "--view")
  }

  if (!is.null(gcs_bucket)) {
    args <- c(args, "--gcs-bucket", shQuote(gcs_bucket))
    if (wait) {
      args <- c(args, "--wait")
    }
  } else if (zip) {
    args <- c(args, "--zip")
  }

  status <- system2(py_exe, args = args, stdout = "", stderr = "")

  if (status != 0) {
    unlink(tmp_in)
    stop("Python extrapolation failed.")
  }

  # Read JSON result
  res <- jsonlite::fromJSON(output_json)

  # Stitch tiles if multiple files exist for any layer (only for local download mode)
  if (!view && is.null(gcs_bucket)) {
    message("Checking for tiled outputs to stitch...")
    layer_keys <- names(res)[sapply(res, is.character)] # Get keys with file paths

    for (key in layer_keys) {
      if (length(res[[key]]) > 1) {
        message("Stitching ", key, " (", length(res[[key]]), " tiles)...")
        # Use terra to mosaic
        tryCatch(
          {
            # Load list as SpatRasterCollection
            s_coll <- terra::sprc(res[[key]])
            # Mosaic them
            mfd <- terra::mosaic(s_coll)

            # Define final output name (base name without tiling suffix)
            # res[[key]][1] looks like "path/name_0_0.tif"
            # We want "path/name.tif"
            final_path <- gsub("_0_0\\.tif$", ".tif", res[[key]][1])

            # Write merged file
            terra::writeRaster(mfd, final_path, overwrite = TRUE)

            # Remove tiles
            unlink(res[[key]])

            # Update res object
            res[[key]] <- final_path
          },
          error = function(e) {
            warning("Stitching failed for ", key, ": ", e$message)
          }
        )
      }
    }
  }

  # Clean up input temp file
  unlink(tmp_in)

  if (!is.null(res$monitoring_url)) {
    message("Monitoring URL: ", res$monitoring_url)
    message("You can also monitor via rgee::ee_monitoring().")
  }

  return(res)
}

#' Predict at Specific Coordinates
#'
#' This function takes a data frame of coordinates and returns predictions
#' (similarity or probability) based on a trained model.
#'
#' @param df Data frame with columns `longitude`, `latitude`, and `year`.
#' @param analysis_meta_path Path to the JSON metadata file generated by `analyze_embeddings`.
#' @param scale Resolution in meters for the Alpha Earth embeddings. Defaults to 10.
#' @param python_path Optional. Path to Python executable.
#' @param gee_project Optional. Google Cloud Project ID.
#' @param coarse_meta_path Optional. Path to 1km coarse metadata for filtering.
#' @return A data frame with original data plus a `similarity` column.
#' @export
predict_at_coords <- function(df, analysis_meta_path, scale = 10, python_path = NULL, gee_project = NULL, coarse_meta_path = NULL) {
  py_exe <- resolve_python_path(python_path)

  if (is.null(py_exe) || !file.exists(py_exe)) {
    stop("Could not find a valid Python environment.")
  }

  tmp_in <- tempfile(fileext = ".csv")
  tmp_out <- tempfile(fileext = ".csv")
  write.csv(df, tmp_in, row.names = FALSE)

  args <- c(
    "-m", "autoSDM.cli", "predict",
    "--input", shQuote(tmp_in),
    "--output", shQuote(tmp_out),
    "--meta", shQuote(analysis_meta_path),
    "--scale", scale
  )

  if (!is.null(gee_project)) args <- c(args, "--project", shQuote(gee_project))
  if (!is.null(coarse_meta_path)) args <- c(args, "--coarse-meta", shQuote(coarse_meta_path))
  if (exists("sa_json_key") && !is.null(sa_json_key)) args <- c(args, "--key", shQuote(sa_json_key))

  status <- system2(py_exe, args = args, stdout = "", stderr = "")

  if (status != 0) {
    unlink(tmp_in)
    unlink(tmp_out)
    stop("Point prediction failed.")
  }

  res <- read.csv(tmp_out)
  unlink(tmp_in)
  unlink(tmp_out)

  return(res)
}
